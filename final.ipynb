{"cells":[{"cell_type":"markdown","metadata":{},"source":["This is the final exam for ARE212, covering material from the second\nhalf of the course taught in Spring 2020.   The exam is\n&ldquo;take-at-home&rdquo;; you may consult any resources you wish in completing\nit (notes, textbooks, lecture videos, etc.) except for other\npeople.  This last restriction isn&rsquo;t easily enforceable; I rely on\nyou to approach this as principled adults who adhere to the\nBerkeley Honor Code.\n\nMore guidance:\n\n-   The exam is due at 11am  on Tuesday May 12.\n-   In completing the exam you should develop written arguments\n    (e.g., expressed using \\LaTeX{} or pencil and paper).  In some cases\n    you may wish to supplement these written arguments with\n    computation, such as Monte Carlo experiments.  Should you do so,\n    please provide me with your working, open source, well-documented code.  (This\n    last could be links to a github repo, a Jupyter notebook attached\n    to an email, or similar).  In any case please be sure that\n    materials you submit are well-organized and clearly\n    documented&#x2014;if I overlook some file you&rsquo;ve sent or can&rsquo;t run it\n    that&rsquo;s on you.\n-   You are welcome to use arguments developed in our `bcourses`\n    discussion, but in this case please clearly cite the person and\n    discussion (e.g., &ldquo;As argued by Ligon in a discussion \\`Tests of\n    Normality&rsquo;\n    ([https://bcourses.berkeley.edu/courses/1487913/discussion_topics/5746331](https://bcourses.berkeley.edu/courses/1487913/discussion_topics/5746331)),\n    the optimal weighting matrix has a great deal of structure than\n    can be exploited.&rdquo;)\n-   Please email files or links to `ligon@berkeley.edu`.\n-   If you have questions about the final I will look for these on\n    [ligonltd.slack.com](https://ligonltd.slack.com), but I do not intend to be continuously\n    available on-line, so much better if you can ask questions early!\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Cross-Validation & Bootstrap Standard Errors\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider estimation of a linear model $y = X\\beta + u$, with the\nidentifying assumption that $\\E(u|X)=0$.  \n\nWhen we compute $K$-fold cross-validation of a tuning parameter &lambda;\n(e.g., the penalty parameter in a LASSO regression), then for each value of\n&lambda; we obtain $K$ estimates of any given parameter, say\n$\\beta_i$; denote the estimates of this parameter by\n$b_{i}^\\cdot=(b_{i}^1,\\dots,b_{i}^K)$.  If our total sample (say\n$D_1$) comprises\n$N$ iid observations, then each of our $K$ estimates will be based\non a sample $D_1^k$ of roughly $N\\frac{K-1}{K}$ observations.\n\n1.  How can you use the estimates $b_{i}^\\cdot$ to estimate the\n    variance of the estimator?\n2.  What can you say about the variance of your estimator of the\n    variance?  In particular, how does it vary with $K$?\n3.  Suppose we use $\\bar{b}(\\lambda)=K^{-1}\\sum_{k=1}^K b^{k}$ as our\n    preferred estimate of $\\beta$ at a given value of the tuning\n    parameter $\\lambda$.  Construct an $R^2$ statistic which maps a\n    sample $D$ and a parameter vector $b$ into $[0,1]$.  Compare the\n    following:\n    1.  $R^2(D_1,\\bar{b}(\\lambda))$ and $R^2(D_1,b_{OLS})$, where\n        $b_{OLS}$ denotes the OLS estimator estimated using the entire\n        sample $D_1$, so that $R^2(D_1,b_{OLS})$ corresponds to the\n        usual least-squares $R^2$ statistic.\n    \n    2.  $R^2(D,\\bar{b}(\\lambda))$ and $R^2(D,b_{OLS})$, where\n        $b_{OLS}$ and $\\bar{b}(\\lambda)$ are estimated using $D_1$ as\n        described above, but where $D$ is some other iid sample from\n        the same data-generating process.\n    \n    3.  $K^{-1}\\sum_{k=1}^K R^2(D_1^k,\\bar{b}(\\lambda))$ and\n        $K^{-1}\\sum_{k=1}^K R^2(D_1^k,b_{OLS})$;\n    \n    4.  $K^{-1}\\sum_{k=1}^K R^2(D_1^k,\\bar{b}(\\lambda))$ and\n        $K^{-1}\\sum_{k=1}^K R^2(D_1^k,b^{k}(\\lambda))$;\n    \n    5.  $R^2(D,\\bar{b}(\\lambda))$ and $R^2(D,\\beta)$;\n    \n    6.  $R^2(D,b_{OLS})$ and $R^2(D,\\beta)$;\n\n4.  How do the $R^2$ statistics you worked with above compare with\n    various notions of mean-square error?  The statistics which rely\n    on $\\beta$ are typically infeasible, so setting these aside, how\n    might you use these statistics to choose a &ldquo;best&rdquo; estimator?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Weighting of Linear IV Estimators\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider the Linear IV model\n$$\n      y = X\\beta + u\\qquad \\E(Z^\\T u)=0.\n  $$\n\n1.  Exploiting the moment condition, under what conditions does the\n    estimator $b_{IV}$ satisfying $Z^\\T y = (Z^\\T X)b_{IV}$\n    consistently estimate $\\beta$?\n2.  Suppose that $Z$ has $\\ell$ columns.  Construct a symmetric,\n    $\\ell\\times\\ell$ full rank matrix $W$, and a corresponding estimator $b_W$\n    satisfying $WZ^\\T y = W(Z^\\T X)b_{W}$.  Under what conditions\n    will this estimator consistently estimate $\\beta$?\n3.  Describe the GMM criterion function that $b_W$ minimizes.\n4.  Consider Hansen&rsquo;s description of the two-stage least squares\n    estimator (Section 12.12).  What is $W$ for this estimator?\n    Under what conditions is this the optimally weighted GMM estimator?\n5.  $W=I$ for the $b_{IV}$ estimator described above.  Under what\n    conditions is $b_{IV}$ the optimally weighted GMM estimator?\n6.  Describe a feasible GMM estimator for this model which is\n    optimally weighted given an iid sampling assumption and a\n    regularity condition that second moments of $(y,X,Z)$ be finite.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## RCT Design\n\n"]},{"cell_type":"markdown","metadata":{},"source":["When designing an RCT (randomized control trial), one important\nelement of the experimental design involves *power calculations*;\nthese in turn rely on pre-specification of the regression one\nproposes to estimate; this regression is generally supposed to\nidentify one or more parameters of interest; often the question the\nexperiment is designed to answer boils down to whether or not this\nparameter is different from zero, which suggests a test statistic\n(typically a $t$ statistic).\n\nSo, one thing that needs to be settled early is how large the\nexperiment needs to be to make the probability of a type II error\nless than some benchmark (typically 20%), holding fixed the\nprobability of a type I error (typically 5%).  A large number of\nexamples can be found at the AEA registry; one interesting case is the\nregistration [https://www.socialscienceregistry.org/trials/1558](https://www.socialscienceregistry.org/trials/1558),\nwhich eventually led to publication as Bandiera et al (2020).  This\ninvolves some randomly assigned *treatment*; in the example given\nthis is a *community-level* treatment involving the establishment of\nclubs for adolescent girls; the (alternative) hypothesis of the\nstudy is that the establishment of such clubs will lead to greater\n&ldquo;economic empowerment&rdquo; for participating girls and &ldquo;greater control\nover their bodies&rdquo;.\n\nThis particular study involved 150 communities, fifty of\nwhich were randomly assigned to be &ldquo;controls&rdquo;, while 100 were randomly\nchosen to have clubs established within them.  Suppose that whether\na girl $j$ lives in a community with a club depends on a binary\ntreatment variable $T_j$.\n\n1.  Suppose that we&rsquo;re interested in the effect of clubs on some\n    outcome $y$, and so wish to estimate the parameter $\\beta_1$ in\n    $y = \\beta_0 + T\\beta_1 + u$.  The random assignment of $T$\n    implies that it is independent of $u$.  Suggest a moment\n    condition that could be exploited to estimate $\\beta_1$.\n2.  Suppose it is known in advance that the variance of $y$ is\n    one.  The registration for this experiment indicates\n    that about 4000 girls lived in treatment communities, while about\n    2000 lived in control communities.  Under an iid sampling\n    assumption, construct a $t$-statistic which could be used to\n    test the hypothesis that the OLS estimate of $\\beta_1$, say\n    $b_{OLS}$, was significantly different from zero.\n3.  Still using the OLS estimator and the iid sampling assumption,\n    what is the &ldquo;minimum detectable effect size&rdquo; allowing for a\n    probability of type I error of 5% and a probability of type II\n    error of 20% (where the absolute value of $\\beta_1$ is\n    interpreted as the &ldquo;effect size&rdquo;)?\n4.  It is unlikely that all girls in treatment communities will\n    actually join the &ldquo;club&rdquo;; instead, each will make a decision\n    about whether to join or not; denote this by $D_j$ equal to one\n    if girl $j$ joins the club, and zero otherwise.  If we&rsquo;re\n    interested in the effects of club participation on outcome $y$ rather than the\n    effects of having a club in the community, this suggests that the\n    equation of interest ought to be something like $y=\\gamma_0 +\n         D\\gamma_1 + v$.  The treatment $T$ is still randomly assigned,\n    though of course $D$ is not; how can this be exploited to obtain\n    estimates of $\\gamma_1$?\n5.  In the registration for the RCT, the researchers proposed using\n    the randomly assigned treatment as an instrument for girls&rsquo;\n    participation decision, and construct a just-identified two-stage\n    least squares estimator of the coefficient corresponding to our\n    $\\gamma_1$ (in the application there are other controls, with\n    which we won&rsquo;t concern ourselves).  What can we say about the\n    distribution of this estimator and distribution of the test\n    statistic you employed to handle the power calculations?  If\n    $\\gamma_1$ is the coefficient of interest, how would you go about\n    re-doing the power calculations?  What are the critical issues,\n    and how could they be addressed?\n6.  In addition to the moment conditions which identify the two-stage\n    least squares estimator, the independence of $T$ implies that\n    there are many more moment conditions which could be exploited.\n    Suggest a *sequence* of possible moment conditions, and indicate\n    a practical estimation strategy which could make efficient use of these.\n7.  The treatment $T$ is randomly assigned to different\n    *communities*; obviously it is not randomly assigned to different\n    *girls* (the correlation between $(T_j,T_{j'})$ is one for girls\n    $(j,j')$ in the same community).  Sketch a causal diagram (a\n    directed graph) illustrating a set of assumptions sufficient for\n    the two-stage least squares estimator to consistently estimate\n    the model parameter $\\gamma_1$.  Comment on the plausibility of\n    these assumptions; are any of these testable?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Nested Samples\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider the linear model $y=X\\beta + u$, where $X$ is thought to\ndepend on $u$, but where we have a set of instruments $Z$ such that\n$\\E Z^\\T u = 0$.  In this case our observations on $y$ are limited,\nin that we don&rsquo;t always observe $y$ even when we do observe\n$(X,Z)$.  We can think of this as having two samples, nested in the\nfollowing way.  We have $N_1$ iid observations on the triple\n$(y,X,Z)_1$ but $N_2>N_1$ iid observations on $(X,Z)_2$, with $(X,Z)_1$\n(i.e., the observations on $X$ and $Z$ in the first dataset) a\nsubset of $(X,Z)_2$.  How can we best make use of all these data?\n\n1.  One econometrician suggests an augmented sort of two-stage-least\n    squares approach, using the richer dataset to estimate a\n    linear relationship $X_2 = Z_2\\pi + v$, and thus constructing a\n    &ldquo;first-stage&rdquo; prediction equation $\\hat{X} = Z\\hat{\\pi}_2$ which\n    is more precisely estimated that it would be in the usual case in\n    which only data in $(y,X,Z)_1$ was exploited.\n    1.  Continue the argument by substituting into the second stage.\n        What can you say about the properties of the augmented\n        estimator compared to the properties of the usual\n        two-stage least squares estimator?\n    2.  Under what conditions would the augmented estimator be\n        preferred to two-stage least squares on just the sample of\n        $N_1$ observations?\n\n2.  A second econometrician suggests using the smaller sample to\n    construct a sample moment condition $(Z_1^\\T y_1)=(Z_1^\\T X_1)b$,\n    and argues that if $b$ in this condition identifies $\\beta$, then\n    it should be possible to construct $\\hat{u}_2 = \\hat{y}_2 -\n         X_2b$, and that for this larger set of observations we must have\n    $\\E Z_2^\\T\\hat{u}_2 = 0$.  She argues that these two sets of moment\n    conditions could then be combined into an over-identified\n    optimally-weighted GMM estimator.\n    1.  How would you construct the optimal GMM weighting matrix for\n        this approach?  Derive an expression for the asymptotic\n        variance matrix for the estimator $b$.  How does it depend on\n        the larger sample?\n    2.  Comment on this approach.  Does the second set of moment\n        conditions add useful information?\n    3.  If you also knew that $u$ was homoskedastic how could you\n        exploit this information?  How would the resulting estimator\n        compare with two-stage-least squares?  What can you say about\n        the relative efficiency of this estimator versus two-stage\n        least squares?\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}