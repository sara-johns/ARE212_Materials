{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Sample Properties of Linear GMM\n",
    "#Ligon Lecture , April 20, 2020\n",
    "#Updated: Sara Johns, April 30, 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM provides a generalized way to think about instrumental\n",
    "variables estimators, and we also have evidence that the finite\n",
    "sample properties of these estimators may be poor.  Here we&rsquo;ll\n",
    "construct a simple Monte Carlo framework within which to evaluate\n",
    "the finite-sample behavior of GMM linear IV estimators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymptotic Variance of GMM estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have $\\mbox{E}g_j(\\beta)g_j(\\beta)^\\top=\\Omega$ and\n",
    "$\\mbox{E}\\frac{\\partial g_j}{\\partial b^\\top}(\\beta)=Q$ then we&rsquo;ve\n",
    "seen that the asymptotic variance of the optimally weighted GMM\n",
    "estimator is\n",
    "$$\n",
    "       V_b = \\left(Q^\\top\\Omega^{-1}Q\\right)^{-1}.\n",
    "   $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generating Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the linear IV model\n",
    "\n",
    "\\begin{align*}\n",
    "   y &= X\\beta + u\\\\\n",
    "   \\mbox{E}Z^\\top u &= 0\n",
    "\\end{align*}\n",
    "\n",
    "Thus, we need to describe processes that generate $(X,Z,u)$.\n",
    "\n",
    "The following code block defines the important parameters governing\n",
    "the DGP; this is the &ldquo;TRUTH&rdquo; we&rsquo;re designing tools to reveal.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, norm\n",
    "import statistics as stats\n",
    "\n",
    "## Play with us!\n",
    "beta = 2    # \"Coefficient of interest\"\n",
    "gamma = 0.7    # Governs effect of u on X\n",
    "sigma_u = 12  # Note assumption of homoskedasticity, \n",
    "              #we started with sigma_u=1, then increased to =10\n",
    "## Play with us!\n",
    "\n",
    "# Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ\n",
    "\n",
    "ell = 4 # Play with me too! Number of instruments\n",
    "\n",
    "# Arbitrary (but deterministic) choice for VXZ\n",
    "A = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1)) \n",
    "\n",
    "\n",
    "## Below here we're less playful.\n",
    "\n",
    "# Var([X,Z]|u) is constructed so that pos. def.\n",
    "VXZ = A.T@A \n",
    "\n",
    "Q = VXZ[1:,[0]]  # EZX'\n",
    "\n",
    "truth = (beta,gamma,sigma_u,VXZ)\n",
    "\n",
    "## But play with Omega if you want to introduce heteroskedascity\n",
    "Omega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')\n",
    "\n",
    "# Asymptotic variance of optimally weighted GMM estimator:\n",
    "print(inv(Q.T@inv(Omega)@Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now code to generate $N$ realizations of $(y,X,Z)$ given some &ldquo;truth&rdquo;\n",
    "$(beta,gamma,sigma_u,VXZ)$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import distributions as iid\n",
    "\n",
    "def dgp(N,beta,gamma,sigma_u,VXZ):\n",
    "    \"\"\"Generate a tuple of (y,X,Z).\n",
    "\n",
    "    Satisfies model:\n",
    "        y = X@beta + u\n",
    "        E Z'u = 0\n",
    "        Var(u) = sigma^2\n",
    "        Cov(X,u) = gamma*sigma_u^2\n",
    "        Var([X,Z}|u) = VXZ\n",
    "        u,X,Z mean zero, Gaussian\n",
    "\n",
    "    Each element of the tuple is an array of N observations.\n",
    "\n",
    "    Inputs include\n",
    "    - beta :: the coefficient of interest\n",
    "    - gamma :: linear effect of disturbance on X\n",
    "    - sigma_u :: Variance of disturbance\n",
    "    - VXZ :: Cov([X,Z|u])\n",
    "    \"\"\"\n",
    "    \n",
    "    u = iid.norm.rvs(size=(N,1))*sigma_u\n",
    "\n",
    "    # \"Square root\" of VXZ via eigendecomposition\n",
    "    lbda,v = np.linalg.eig(VXZ)\n",
    "    SXZ = v@np.diag(np.sqrt(lbda))\n",
    "\n",
    "    # Generate normal random variates [X*,Z]\n",
    "    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n",
    "\n",
    "    # But X is endogenous...\n",
    "    X = XZ[:,[0]] + gamma*u\n",
    "    Z = XZ[:,1:]\n",
    "\n",
    "    # Calculate y\n",
    "    y = X*beta + u\n",
    "\n",
    "    return y,X,Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on DGP:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "data = dgp(N,*truth)\n",
    "\n",
    "y,X,Z = data # Unpack tuple to check on things\n",
    "\n",
    "# Check that we've computed things correctly:\n",
    "print(VXZ)\n",
    "\n",
    "print(np.cov(np.c_[X,Z].T) - VXZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a data-generating process we proceed with under\n",
    "   the conceit that we can observe samples generated by this process,\n",
    "   but otherwise temporarily &ldquo;forget&rdquo; the properties of the DGP, and use the\n",
    "   generated data to try to reconstruct aspects of the DGP.\n",
    "\n",
    "In our example, we consider using the optimally weighted linear IV\n",
    "estimator, and define a function which computes observation-level\n",
    "deviations from expectations for this model. To testimate a different\n",
    "model this is the function we&rsquo;d want to re-define.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gj(b,y,X,Z):\n",
    "    \"\"\"Observations of g_j(b).\n",
    "\n",
    "    This defines the deviations from the predictions of our model; i.e.,\n",
    "    e_j = Z_ju_j, where EZ_ju_j=0.\n",
    "\n",
    "    Can replace this function to testimate a different model.\n",
    "    \"\"\"\n",
    "    return Z*(y - X*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct sample moments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by defining a function to construct the sample moments given\n",
    "    the data and a parameter estimate $b$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gN(b,data):\n",
    "    \"\"\"Averages of g_j(b).\n",
    "\n",
    "    This is generic for data, to be passed to gj.\n",
    "    \"\"\"\n",
    "    e = gj(b,*data)\n",
    "\n",
    "    # Check to see more obs. than moments.\n",
    "    assert e.shape[0] > e.shape[1]\n",
    "    \n",
    "    return e.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define estimator of Egg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to compute covariance matrix of moments.\n",
    "Re-centering can be important in finite samples, even if irrelevant in\n",
    "the limit.  Since we have $\\mbox{E}g_j(\\beta)=0$ under the null we may\n",
    "as well use this information when constructing our weighting matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Omegahat(b,data):\n",
    "    e = gj(b,*data)\n",
    "\n",
    "    # Recenter! We have Eu=0 under null.\n",
    "    # Important to use this information. centering is important\n",
    "    e = e - e.mean(axis=0) \n",
    "    #could have writen return np.cov(e) divided by N e.shape[0]\n",
    "    return e.T@e/e.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check construction of criterion given our data.  We want\n",
    "something that looks nice and quadratic, at least in the neighborhood\n",
    "of $\\beta$.\n",
    "\n",
    "Check construction of weighting matrix for our data at true parameter $\\beta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Winv = Omegahat(beta,data) \n",
    "print(Winv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the criterion function given a weighting matrix $W$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(b,W,data):\n",
    "\n",
    "    m = gN(b,data) # Sample moments m  @ b\n",
    "    N = data[0].shape[0]  # number of observ\n",
    "\n",
    "    return N*m.T@W@m # Scale by sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check construction of criterion given our data.  We want\n",
    "something that looks nice and quadratic, at least in the neighborhood\n",
    "of $\\beta$.  Note that comparing the criterion to the critical values\n",
    "of the $\\chi^2$ statistic gives us an alternative way to construct\n",
    "confidence intervals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Limiting distribution of criterion (under null)\n",
    "limiting_J = iid.chi2(ell-1)\n",
    "\n",
    "# Limiting SE of b should be in theory\n",
    "sigma_0 = np.sqrt(inv(Q.T@inv(Winv)@Q)/N)[0][0] \n",
    "\n",
    "# Choose 8 sigma_0 neighborhood of \"truth\"\n",
    "B = np.linspace(beta-4*sigma_0,beta+4*sigma_0,100)\n",
    "W = inv(Winv)\n",
    "\n",
    "#blue is the criterion function\n",
    "#red line is the critical value of chi square stat\n",
    "\n",
    "_ = plt.plot(B,[J(b,W,data) for b in B.tolist()])\n",
    "_ = plt.axhline(limiting_J.isf(0.05),color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Step Estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next implement the two-step GMM estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def two_step_gmm(data):\n",
    "\n",
    "    # First step uses identity weighting matrix\n",
    "    W1 = np.eye(gj(1,*data).shape[1])\n",
    "\n",
    "    b1 = minimize_scalar(lambda b: J(b,W1,data)).x \n",
    "\n",
    "    # Construct 2nd step weighting matrix using\n",
    "    # first step estimate of beta\n",
    "    W2 = inv(Omegahat(b1,data))\n",
    "\n",
    "    return minimize_scalar(lambda b: J(b,W2,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let&rsquo;s try it with an actual sample, just to see that things work:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soltn = two_step_gmm(data)\n",
    "\n",
    "print(\"b=%f, J=%f, Critical J=%f\" % (soltn.x,soltn.fun,limiting_J.isf(0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterated GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next implement the iterated GMM estimator. This is similar to two step, but we will continue the process until we get convergence. There was discussion about whether convergence means convergence in b or convergence in $\\hat{\\Omega}$. I referenced Hansen, Heaton, and Yaron (1996) which uses convergence in b and so that is what I do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterated_gmm(data, epsilon):\n",
    "\n",
    "    # First use identity weighting matrix\n",
    "    W1 = np.eye(gj(1,*data).shape[1])\n",
    "    \n",
    "    # set initial b_1, b_2 for while loop\n",
    "    b_1 = 10\n",
    "    b_2 = 0\n",
    "    \n",
    "    j = 1\n",
    "    \n",
    "    # Now iterate: will continue until norm is less than some epsilon\n",
    "    while norm(b_2 - b_1) > epsilon:\n",
    "        \n",
    "        j += 1\n",
    "        \n",
    "        # calculate b1 \n",
    "        b_1 = minimize_scalar(lambda b: J(b,W1,data)).x\n",
    "\n",
    "        # Update weighting matrix\n",
    "        W1 = inv(Omegahat(b_1,data))\n",
    "        \n",
    "        # calculate b2\n",
    "        b_2 = minimize_scalar(lambda b: J(b,W1,data)).x\n",
    "    \n",
    "        if j > 10000:\n",
    "            break\n",
    "    \n",
    "    # get all the stuff we want\n",
    "    b_final = minimize_scalar(lambda b: J(b,W1,data))\n",
    "\n",
    "    return(b_final) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soltn_itt = iterated_gmm(data, 0.00001)\n",
    "\n",
    "print(\"b=%f, J=%f, Critical J=%f\" % (soltn_itt.x,soltn_itt.fun,limiting_J.isf(0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuously updated GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the weighting matrix as a function of b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_cu(b,data):\n",
    "\n",
    "    m = gN(b,data) # Sample moments m  @ b\n",
    "    N = data[0].shape[0]  # number of observ\n",
    "    \n",
    "    # continuously updating minimizes the objective with a weighting matrix that is a function of b\n",
    "    return N*m.T@inv(Omegahat(b,data))@m # Scale by sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soltn_cu = minimize_scalar(lambda b: J_cu(b,data))\n",
    "\n",
    "print(\"b=%f, J=%f, Critical J=%f\" % (soltn_cu.x,soltn_cu.fun,limiting_J.isf(0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our experiment begins.  We set our frequentist hats firmly on our\n",
    "heads, and draw repeated samples of data, each generating a\n",
    "corresponding estimate of beta.  Then the empirical distribution of\n",
    "these samples tells us about the *finite* sample performance of our estimator.\n",
    "\n",
    "We&rsquo;ll generate a sample of estimates of $b$ by drawing repeated\n",
    "samples of size $N$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 1000 # Sample size\n",
    "\n",
    "D = 1000 # Monte Carlo draws\n",
    "\n",
    "b_draws_2s = []\n",
    "J_draws_2s = []\n",
    "b_draws_itt = []\n",
    "J_draws_itt = []\n",
    "b_draws_cu = []\n",
    "J_draws_cu = []\n",
    "for d in range(D):\n",
    "    mc_data = dgp(N,*truth)\n",
    "    soltn = two_step_gmm(mc_data)\n",
    "    b_draws_2s.append(soltn.x)\n",
    "    J_draws_2s.append(soltn.fun)\n",
    "    \n",
    "    soltn_itt = iterated_gmm(mc_data, 0.00001)\n",
    "    b_draws_itt.append(soltn_itt.x)\n",
    "    J_draws_itt.append(soltn_itt.fun)\n",
    "    \n",
    "    soltn_cu = minimize_scalar(lambda b: J_cu(b,mc_data))\n",
    "    b_draws_cu.append(soltn_cu.x)\n",
    "    J_draws_cu.append(soltn_cu.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for histograms (thanks to Connor):\n",
    "def graph_hist(data, caption):\n",
    "    \"\"\"Plot a histogram of the relevant data\"\"\"\n",
    "    plt.hist(data, bins='auto')\n",
    "    plt.title(caption)\n",
    "    plt.axvline(stats.mean(data), color='k', linestyle='dashed', linewidth=1)\n",
    "    plt.axvline(stats.median(data), color='w', linestyle='dashed', linewidth=1)\n",
    "    plt.axvline(beta, color='r', linestyle='dashed', linewidth=1)\n",
    "    plt.show()\n",
    "    print(\"Estimate:\", stats.mean(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot two-step GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(b_draws_2s,\"Distribution of b: two-step GMM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot iterated GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(b_draws_itt,\"Distribution of b: iterated GMM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot continuously updated GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hist(b_draws_cu,\"Distribution of b: continuously updated GMM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Monte Carlo draws vs. Asymptotic distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Monte Carlo standard errors with asymptotic approximation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting distribution of estimator\n",
    "\n",
    "limiting_b = iid.norm(scale=sigma_0)\n",
    "\n",
    "print(\"Bootstrapped standard errors two step: %g\" % np.std(b_draws_2s))\n",
    "print(\"Bootstrapped standard errors iterated: %g\" % np.std(b_draws_itt))\n",
    "print(\"Bootstrapped standard errors CU: %g\" % np.std(b_draws_cu))\n",
    "print(\"Asymptotic approximation: %g\" % sigma_0)\n",
    "print(\"Critical value for J statistic: %g (5%%)\" % limiting_J.isf(.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct probability plot (bootstrapped $b$s vs. quantiles of\n",
    "limiting distribution) - two step GMM:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import probplot\n",
    "\n",
    "_ = probplot(b_draws_2s,dist=limiting_b,fit=False,plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for iterated GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = probplot(b_draws_itt,dist=limiting_b,fit=False,plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for continuously updated GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = probplot(b_draws_cu,dist=limiting_b,fit=False,plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider the a $p$-$p$ plot for $J$ statistics (recall these\n",
    "should be distributed $\\chi^2_{\\ell-1}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppplot(data,dist):\n",
    "    data = np.array(data)\n",
    "\n",
    "    # Theoretical CDF, evaluated at points of data\n",
    "    P = [dist.cdf(x) for x in data.tolist()]\n",
    "\n",
    "    # Empirical CDF, evaluated at points of data\n",
    "    Phat = [(data<x).mean() for x in data.tolist()]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.scatter(P,Phat)\n",
    "    ax.plot([0,1],[0,1],color='r') # Plot 45\n",
    "    ax.set_xlabel('Theoretical Distribution')\n",
    "    ax.set_ylabel('Empirical Distribution')\n",
    "    ax.set_title('p-p Plot')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-step GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = ppplot(J_draws_2s, limiting_J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For iterated GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ppplot(J_draws_itt, limiting_J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuously updated GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = ppplot(J_draws_cu, limiting_J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "With small sample size, overidentification ($\\ell > k$), and large variance (high $\\sigma_u$) we see that\n",
    "- Continuously updated GMM has less bias, but fatter tails\n",
    "- The limiting distributions of b are similar\n",
    "- The J statistic is better for continuously updated GMM, better to use to test for overidentifying restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes about other parameters tested:\n",
    "\n",
    "1. In just identified case, methods are the same.\n",
    "2. Small variance tends to give us nice results for all methods.\n",
    "3. Similarly, large sample size (even when variance is high) gives us nice results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
