{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Sample Properties of Linear GMM\n",
    "#Ligon Lecture , April 20, 2020\n",
    "#Updated: Sara Johns, April 28, 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM provides a generalized way to think about instrumental\n",
    "variables estimators, and we also have evidence that the finite\n",
    "sample properties of these estimators may be poor.  Here we&rsquo;ll\n",
    "construct a simple Monte Carlo framework within which to evaluate\n",
    "the finite-sample behavior of GMM linear IV estimators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymptotic Variance of GMM estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have $\\mbox{E}g_j(\\beta)g_j(\\beta)^\\top=\\Omega$ and\n",
    "$\\mbox{E}\\frac{\\partial g_j}{\\partial b^\\top}(\\beta)=Q$ then we&rsquo;ve\n",
    "seen that the asymptotic variance of the optimally weighted GMM\n",
    "estimator is\n",
    "$$\n",
    "       V_b = \\left(Q^\\top\\Omega^{-1}Q\\right)^{-1}.\n",
    "   $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generating Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the linear IV model\n",
    "\n",
    "\\begin{align*}\n",
    "   y &= X\\beta + u\\\\\n",
    "   \\mbox{E}Z^\\top u &= 0\n",
    "\\end{align*}\n",
    "\n",
    "Thus, we need to describe processes that generate $(X,Z,u)$.\n",
    "\n",
    "The following code block defines the important parameters governing\n",
    "the DGP; this is the &ldquo;TRUTH&rdquo; we&rsquo;re designing tools to reveal.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "## Play with us!\n",
    "beta = 1     # \"Coefficient of interest\"\n",
    "gamma = 1    # Governs effect of u on X\n",
    "sigma_u = 10  # Note assumption of homoskedasticity, \n",
    "              #we started with sigma_u=1, then increased to =10\n",
    "## Play with us!\n",
    "\n",
    "# Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ\n",
    "\n",
    "ell = 1 # Play with me too! Number of instruments\n",
    "\n",
    "# Arbitrary (but deterministic) choice for VXZ\n",
    "A = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1)) \n",
    "\n",
    "\n",
    "## Below here we're less playful.\n",
    "\n",
    "# Var([X,Z]|u) is constructed so that pos. def.\n",
    "VXZ = A.T@A \n",
    "\n",
    "Q = VXZ[1:,[0]]  # EZX'\n",
    "\n",
    "truth = (beta,gamma,sigma_u,VXZ)\n",
    "\n",
    "## But play with Omega if you want to introduce heteroskedascity\n",
    "Omega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')\n",
    "\n",
    "# Asymptotic variance of optimally weighted GMM estimator:\n",
    "print(inv(Q.T@inv(Omega)@Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now code to generate $N$ realizations of $(y,X,Z)$ given some &ldquo;truth&rdquo;\n",
    "$(beta,gamma,sigma_u,VXZ)$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import distributions as iid\n",
    "\n",
    "def dgp(N,beta,gamma,sigma_u,VXZ):\n",
    "    \"\"\"Generate a tuple of (y,X,Z).\n",
    "\n",
    "    Satisfies model:\n",
    "        y = X@beta + u\n",
    "        E Z'u = 0\n",
    "        Var(u) = sigma^2\n",
    "        Cov(X,u) = gamma*sigma_u^2\n",
    "        Var([X,Z}|u) = VXZ\n",
    "        u,X,Z mean zero, Gaussian\n",
    "\n",
    "    Each element of the tuple is an array of N observations.\n",
    "\n",
    "    Inputs include\n",
    "    - beta :: the coefficient of interest\n",
    "    - gamma :: linear effect of disturbance on X\n",
    "    - sigma_u :: Variance of disturbance\n",
    "    - VXZ :: Cov([X,Z|u])\n",
    "    \"\"\"\n",
    "    \n",
    "    u = iid.norm.rvs(size=(N,1))*sigma_u\n",
    "\n",
    "    # \"Square root\" of VXZ via eigendecomposition\n",
    "    lbda,v = np.linalg.eig(VXZ)\n",
    "    SXZ = v@np.diag(np.sqrt(lbda))\n",
    "\n",
    "    # Generate normal random variates [X*,Z]\n",
    "    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n",
    "\n",
    "    # But X is endogenous...\n",
    "    X = XZ[:,[0]] + gamma*u\n",
    "    Z = XZ[:,1:]\n",
    "\n",
    "    # Calculate y\n",
    "    y = X*beta + u\n",
    "\n",
    "    return y,X,Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on DGP:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "data = dgp(N,*truth)\n",
    "\n",
    "y,X,Z = data # Unpack tuple to check on things\n",
    "\n",
    "# Check that we've computed things correctly:\n",
    "print(VXZ)\n",
    "\n",
    "print(np.cov(np.c_[X,Z].T) - VXZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a data-generating process we proceed with under\n",
    "   the conceit that we can observe samples generated by this process,\n",
    "   but otherwise temporarily &ldquo;forget&rdquo; the properties of the DGP, and use the\n",
    "   generated data to try to reconstruct aspects of the DGP.\n",
    "\n",
    "In our example, we consider using the optimally weighted linear IV\n",
    "estimator, and define a function which computes observation-level\n",
    "deviations from expectations for this model. To testimate a different\n",
    "model this is the function we&rsquo;d want to re-define.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gj(b,y,X,Z):\n",
    "    \"\"\"Observations of g_j(b).\n",
    "\n",
    "    This defines the deviations from the predictions of our model; i.e.,\n",
    "    e_j = Z_ju_j, where EZ_ju_j=0.\n",
    "\n",
    "    Can replace this function to testimate a different model.\n",
    "    \"\"\"\n",
    "    return Z*(y - X*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct sample moments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by defining a function to construct the sample moments given\n",
    "    the data and a parameter estimate $b$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gN(b,data):\n",
    "    \"\"\"Averages of g_j(b).\n",
    "\n",
    "    This is generic for data, to be passed to gj.\n",
    "    \"\"\"\n",
    "    e = gj(b,*data)\n",
    "\n",
    "    # Check to see more obs. than moments.\n",
    "    assert e.shape[0] > e.shape[1]\n",
    "    \n",
    "    return e.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define estimator of Egg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to compute covariance matrix of moments.\n",
    "Re-centering can be important in finite samples, even if irrelevant in\n",
    "the limit.  Since we have $\\mbox{E}g_j(\\beta)=0$ under the null we may\n",
    "as well use this information when constructing our weighting matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Omegahat(b,data):\n",
    "    e = gj(b,*data)\n",
    "\n",
    "    # Recenter! We have Eu=0 under null.\n",
    "    # Important to use this information. centering is important\n",
    "    e = e - e.mean(axis=0) \n",
    "    #could have writen return np.cov(e) divided by N e.shape[0]\n",
    "    return e.T@e/e.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check construction of criterion given our data.  We want\n",
    "something that looks nice and quadratic, at least in the neighborhood\n",
    "of $\\beta$.\n",
    "\n",
    "Check construction of weighting matrix for our data at true parameter $\\beta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Winv = Omegahat(beta,data) \n",
    "print(Winv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the criterion function given a weighting matrix $W$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(b,W,data):\n",
    "\n",
    "    m = gN(b,data) # Sample moments m  @ b\n",
    "    N = data[0].shape[0]  # number of observ\n",
    "\n",
    "    return N*m.T@W@m # Scale by sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check construction of criterion given our data.  We want\n",
    "something that looks nice and quadratic, at least in the neighborhood\n",
    "of $\\beta$.  Note that comparing the criterion to the critical values\n",
    "of the $\\chi^2$ statistic gives us an alternative way to construct\n",
    "confidence intervals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Limiting distribution of criterion (under null)\n",
    "limiting_J = iid.chi2(ell-1)\n",
    "\n",
    "# Limiting SE of b should be in theory\n",
    "sigma_0 = np.sqrt(inv(Q.T@inv(Winv)@Q)/N)[0][0] \n",
    "\n",
    "# Choose 8 sigma_0 neighborhood of \"truth\"\n",
    "B = np.linspace(beta-4*sigma_0,beta+4*sigma_0,100)\n",
    "W = inv(Winv)\n",
    "\n",
    "#blue is the criterion function\n",
    "#red line is the critical value of chi square stat\n",
    "\n",
    "_ = plt.plot(B,[J(b,W,data) for b in B.tolist()])\n",
    "_ = plt.axhline(limiting_J.isf(0.05),color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Step Estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next implement the two-step GMM estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def two_step_gmm(data):\n",
    "\n",
    "    # First step uses identity weighting matrix\n",
    "    W1 = np.eye(gj(1,*data).shape[1])\n",
    "\n",
    "    b1 = minimize_scalar(lambda b: J(b,W1,data)).x \n",
    "\n",
    "    # Construct 2nd step weighting matrix using\n",
    "    # first step estimate of beta\n",
    "    W2 = inv(Omegahat(b1,data))\n",
    "\n",
    "    return minimize_scalar(lambda b: J(b,W2,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let&rsquo;s try it with an actual sample, just to see that things work:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soltn = two_step_gmm(data)\n",
    "\n",
    "print(\"b=%f, J=%f, Critical J=%f\" % (soltn.x,soltn.fun,limiting_J.isf(0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterated GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next implement the iterated GMM estimator. This is similar to two step, but we will continue the process until we get convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterated_gmm(data):\n",
    "\n",
    "    epsilon = 0.00001 # is there convention for what this epsilon should be?\n",
    "\n",
    "    b_start = 10 # to have b's to start with\n",
    "    b_end = 0\n",
    "    \n",
    "    # First use identity weighting matrix\n",
    "    W1 = np.eye(gj(1,*data).shape[1])\n",
    "    \n",
    "    # Now iterate\n",
    "    while b_start - b_end > epsilon:\n",
    "\n",
    "        b_start = minimize_scalar(lambda b: J(b,W1,data)).x\n",
    "\n",
    "        # Update weighting matrix\n",
    "        W1 = inv(Omegahat(b_start,data))\n",
    "    \n",
    "        b_end = minimize_scalar(lambda b: J(b,W1,data)).x\n",
    "    \n",
    "    # get all the stuff we want\n",
    "    b_final = minimize_scalar(lambda b: J(b,W1,data))\n",
    "\n",
    "    return(b_final) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soltn_itt = iterated_gmm(data)\n",
    "\n",
    "print(\"b=%f, J=%f, Critical J=%f\" % (soltn_itt.x,soltn_itt.fun,limiting_J.isf(0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuously updated GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the weighting matrix as a function of b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_cu(b,data):\n",
    "\n",
    "    m = gN(b,data) # Sample moments m  @ b\n",
    "    N = data[0].shape[0]  # number of observ\n",
    "    \n",
    "    return N*m.T@inv(Omegahat(b,data))@m # Scale by sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soltn_cu = minimize_scalar(lambda b: J_cu(b,data))\n",
    "\n",
    "print(\"b=%f, J=%f, Critical J=%f\" % (soltn_cu.x,soltn_cu.fun,limiting_J.isf(0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our experiment begins.  We set our frequentist hats firmly on our\n",
    "heads, and draw repeated samples of data, each generating a\n",
    "corresponding estimate of beta.  Then the empirical distribution of\n",
    "these samples tells us about the *finite* sample performance of our estimator.\n",
    "\n",
    "We&rsquo;ll generate a sample of estimates of $b$ by drawing repeated\n",
    "samples of size $N$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 1000 # Sample size\n",
    "\n",
    "D = 1000 # Monte Carlo draws\n",
    "\n",
    "b_draws_2s = []\n",
    "J_draws_2s = []\n",
    "b_draws_itt = []\n",
    "J_draws_itt = []\n",
    "b_draws_cu = []\n",
    "J_draws_cu = []\n",
    "for d in range(D):\n",
    "    soltn = two_step_gmm(dgp(N,*truth))\n",
    "    b_draws_2s.append(soltn.x)\n",
    "    J_draws_2s.append(soltn.fun)\n",
    "    \n",
    "    soltn_itt = iterated_gmm(dgp(N,*truth))\n",
    "    b_draws_itt.append(soltn_itt.x)\n",
    "    J_draws_itt.append(soltn_itt.fun)\n",
    "    \n",
    "    soltn_cu = minimize_scalar(lambda b: J_cu(b,dgp(N,*truth)))\n",
    "    b_draws_cu.append(soltn_cu.x)\n",
    "    J_draws_cu.append(soltn_cu.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(b_draws_2s,bins=int(np.ceil(np.sqrt(N))))\n",
    "_ = plt.axvline(beta,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(b_draws_itt,bins=int(np.ceil(np.sqrt(N))))\n",
    "_ = plt.axvline(beta,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(b_draws_cu,bins=int(np.ceil(np.sqrt(N))))\n",
    "_ = plt.axvline(beta,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Monte Carlo draws vs. Asymptotic distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Monte Carlo standard errors with asymptotic approximation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting distribution of estimator\n",
    "\n",
    "limiting_b = iid.norm(scale=sigma_0)\n",
    "\n",
    "print(\"Bootstrapped standard errors: %g\" % np.std(b_draws))\n",
    "print(\"Asymptotic approximation: %g\" % sigma_0)\n",
    "print(\"Critical value for J statistic: %g (5%%)\" % limiting_J.isf(.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct probability plot (bootstrapped $b$s vs. quantiles of\n",
    "limiting distribution):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import probplot\n",
    "\n",
    "_ = probplot(b_draws,dist=limiting_b,fit=False,plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider the a $p$-$p$ plot for $J$ statistics (recall these\n",
    "should be distributed $\\chi^2_{\\ell-1}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppplot(data,dist):\n",
    "    data = np.array(data)\n",
    "\n",
    "    # Theoretical CDF, evaluated at points of data\n",
    "    P = [dist.cdf(x) for x in data.tolist()]\n",
    "\n",
    "    # Empirical CDF, evaluated at points of data\n",
    "    Phat = [(data<x).mean() for x in data.tolist()]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.scatter(P,Phat)\n",
    "    ax.plot([0,1],[0,1],color='r') # Plot 45\n",
    "    ax.set_xlabel('Theoretical Distribution')\n",
    "    ax.set_ylabel('Empirical Distribution')\n",
    "    ax.set_title('p-p Plot')\n",
    "\n",
    "    return ax\n",
    "    \n",
    "_ = ppplot(J_draws, limiting_J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
