{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Zeros in expenditure data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider the following data from Uganda, collected at the household\nlevel.  The data itself is *recall* data; the respondent is asked to\nrecall the value, the quantity, and the price of consumption out of\nexpenditures over the past week, for a rather long list of possible\nnon-durable expenditure items.  I&rsquo;ve organized the data as an array,\nwith each row corresponding to a household, and each column\ncorresponding to a different consumption item.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n\nx = pd.read_pickle('uganda_expenditures.pickle')"]},{"cell_type":"markdown","metadata":{},"source":["One thing to note about these data is the large number of &ldquo;zeros&rdquo;.\n  This may reflect the fact that few households consume all different\n  kinds of consumption goods every week, or could reflect &ldquo;missing&rdquo;\n  data on non-zero expenditures (e.g., if the respondent forgot).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Count of non-missing observations by year (t) and market (mkt)\nx.groupby(['t','mkt']).count().T"]},{"cell_type":"markdown","metadata":{},"source":["Missing data can cause serious problems in a demand analysis,\n   depending on how and why data might be missing.  If observations\n   are &ldquo;missing at random&rdquo; (MAR) then it may be an easy issue to\n   address, but if the probability of being missing is related to the\n   disturbance term in the demand equation this becomes a sort of\n   selection problem that will complicate estimation and inference.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Household characteristics\n\n"]},{"cell_type":"markdown","metadata":{},"source":["One class of variables that may help to explain zeros are\n   &ldquo;household characteristics&rdquo;; this includes household size and\n   composition (both because this affects demand and perhaps because\n   there are more potential shoppers); whether a household is urban or\n   rural, and perhaps other characteristics.\n\nHere are some characteristics for the households in Uganda:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["z = pd.read_pickle('uganda_hh_characteristics.pickle')\nz"]},{"cell_type":"markdown","metadata":{},"source":["### Data mining\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Unfortunately, demand theory doesn&rsquo;t offer much guidance to let us\n   know how household characteristics should be related to the\n   probability of a goods&rsquo; consumption being positive in a given week;\n   this is a case where a certain amount of &ldquo;data mining&rdquo; may be a\n   reasonable approach.\n\nWe&rsquo;ll use tools we&rsquo;ve discussed in class, relying on an\nimplementation given by the `scikit.learn` project.  In the first\ninstance, let&rsquo;s consider simply estimating a logit, where the\ndependent variable is simply a dummy indicating that the\nexpenditure of a given good $i$ for a household $j$ at time $t$ is\npositive, and where the right-hand-side variables are all the\nhousehold characteristics in `z`, combined with a collection of\ntime dummies (which we can think of as picking up the influence of\nprices, among other things):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n\ntime_effects = pd.get_dummies(z.reset_index()[['t']].set_index(z.index),columns=['t'])\n\nX = pd.concat([z,time_effects],axis=1).dropna(how='any') # Drop missing data\nx = x.dropna(how='all',axis=1)\n\n# Here's a good place to limit the number of dependent variables\n# if we want to save time.  We select just the first few columns:\nx = x.iloc[:,:5]\n\nEsts = {}\nfor item in x: # Iterate over dummies indicating positive expenditure\n    y = (x>0)[item]  # Dummy for non-missing item expenditures\n    Ests[item] = LogisticRegression(fit_intercept=False,penalty='none').fit(X,y)"]},{"cell_type":"markdown","metadata":{},"source":["#### Coefficients\n\n"]},{"cell_type":"markdown","metadata":{},"source":["This gives us a vector of coefficients for each good, which we can\nre-arrange into a pandas DataFrame.  Recall that in the logit model\n$e^{X\\beta}$ is interpreted as the *odds*.  Thus, for a variable in\n$X$ which is itself a logarithm, like log HSize, the associated\ncoefficient can be interpreted as an elasticity.  Accordingly, if the\ncoefficient on log HSize in the regression involving Matoke is 0.6,\nthen we can say that for every one percent increase in household size\n(other things equal) there&rsquo;s roughly a 0.6% increase in the odds of\nobserving positive Matoke consumption.  \n\nCoefficients associated with variables in levels have the\ninterpetation of *semi-elasticities*; thus, the odds of a rural\nhousehold consuming Matoke are approximately 53% less than that for\nthe average household in the sample.  What is the interpretation of\nthe coefficients associated with discrete counts of different\nhousehold members?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["Coefs = pd.DataFrame({i:Ests[i].coef_.squeeze() for i in Ests.keys()},index=X.columns)\nCoefs"]},{"cell_type":"markdown","metadata":{},"source":["#### Cross-Validation & Lasso\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Interpreting the coefficients above allows us to think about how\ndifferences in household characteristics affect the odds of consuming\na particular good, but our original concern was that the data might\nnot be *missing at random*, which could complicate subsequent\nestimation of a demand system.  \n\nHere we use Lasso & cross-validation to tune the Lasso penalty\nparameter to check which (if any) of our regressors is useful for\nout-of-sample prediction.  \n\nWe again use a canned routine from sklearn, `LogisticRegressionCV`.\nThis bundles both the Lasso penalty criterion and cross-validation\ntogether for us, and searches over a list of penalty parameters to\nminimize the EMSE, computed via $K$-fold cross-validation.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegressionCV\nimport numpy as np\n\nLambdas = np.logspace(-5,5,11)\n\nCVEsts = {}\nfor item in x: # Iterate over dummies indicating positive expenditure\n    print(item)\n    y = (x>0)[item]  # Dummy for non-missing item expenditures\n\n    # Use 5-fold cross-validation in computing CV statistics; using\n    # penalty 'l1' implies a lasso estimator.\n    CVEsts[item] = LogisticRegressionCV(fit_intercept=False,\n                                        Cs = 1/Lambdas,        # Penalty 1/lambdas to search over\n                                        cv=5,                 # K folds\n                                        penalty='l1',         # Lasso penalty\n                                        solver='liblinear',\n                                        scoring='neg_mean_squared_error', # (minus) our CV statistic\n                                        n_jobs=-1             # Number of cores to use (-1=all)\n                                       ).fit(X,y)\n\nCVCoefs = pd.DataFrame({i:CVEsts[i].coef_.squeeze() for i in CVEsts.keys()},index=X.columns)\nCVCoefs"]},{"cell_type":"markdown","metadata":{},"source":["We can see how the estimated coefficients vary with different choices\nof the penalty parameter $\\lambda$ ($=1/C$).  Consider just the\ncoefficients associated with estimation of the Matoke logit: If we try\n$P$ different values of the penalty parameter using $K$-fold\ncross-validation this will be $KP$ different estimates for every\nparameter.  We can average over the $K$ different folds to get a\nclearer picture of how coefficients vary with &lambda;\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["pd.DataFrame(CVEsts['Matoke'].coefs_paths_[True].mean(axis=0),index=Lambdas.tolist(),columns=X.columns).T"]},{"cell_type":"markdown","metadata":{},"source":["and see also how the EMSE varies with $\\lambda$\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["EMSEs={k:-e.scores_[True].mean(axis=0).ravel() for k,e in CVEsts.items()} \n\nEMSEs = pd.DataFrame(EMSEs,index=np.log(Lambdas).tolist()).T\nEMSEs"]},{"cell_type":"markdown","metadata":{},"source":["Plotting these versus $\\log\\lambda$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["EMSEs.T.plot()"]},{"cell_type":"markdown","metadata":{},"source":["Finding the minima of these curves gives estimates of the optimal\n&lambda;:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["lambda_star = pd.Series({k:1/e.C_[0] for k,e in CVEsts.items()})\nlambda_star"]},{"cell_type":"markdown","metadata":{},"source":["Large values of &lambda; encourage parsimony in the selection of\nregressors, so it&rsquo;s not surprising to find that consumption items with\nlarge values of $\\lambda^*$  also have few regressors (this is the\nmagic of Lasso):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["Lasso_outcomes = pd.DataFrame({'#Regressors':(np.abs(CVCoefs)>1e-5).sum(),\n                               'Î»*':lambda_star})\nLasso_outcomes"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}